apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: "{{ .Values.name }}"
  annotations:
    # comma separated list of users that can access this workstation as read only.
    juno-innovations.com/shared: "none"
    juno-innovations.com/kuiper-state: "{{ .Values._kuiper }}"
    juno-innovations.com/workload: "Application"  # mark this as a workload and it's type for the
  labels:
    juno-innovations.com/app: "polaris"
    juno-innovations.com/workstation: "{{ .Values.name }}"
    juno-innovations.com/user: "{{ .Values.user }}"
    juno-innovations.com/session: "{{ .Values.session }}"

spec:
  replicas: 1

  selector:
    matchLabels:
      juno-innovations.com/workstation: "{{ .Values.name }}"

  template:
    metadata:
      labels:
        juno-innovations.com/app: polaris
        juno-innovations.com/user: "{{ .Values.user }}"
        juno-innovations.com/workstation: "{{ .Values.name }}"
      annotations:
        # This will block the cluster autoscaler from evicting this pod in the case
        # it can move to a cheaper instance. This does come at a cost, but ideally
        # this should stop from a workstation getting ripped out from under existing users.
        cluster-autoscaler.kubernetes.io/safe-to-evict: "false"

    spec:
      automountServiceAccountToken: false
      hostname: "{{ .Values.name }}"

      # workstation nodes will require juno-innovations.com/workstation: "true"
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
              - matchExpressions:
                  - key: juno-innovations.com/workstation
                    operator: In
                    values:
                      - "true"
                  {{- if .Values.selector }}
                  {{- range .Values.selector }}
                  - key: {{ .key | quote }}
                    operator: In
                    values:
                      - {{ .value | quote }}
                  {{- end }}
                  {{- end }}

      # allow for this pod to work on nodes reserved for workstations
      tolerations:
        - key: "juno-innovations.com/workstation"
          operator: "Exists"
          effect: "NoSchedule"

      {{- if .Values.gpu }}
      runtimeClassName: nvidia
      {{- end }}

      {{- if .Values.pullSecret }}
      imagePullSecrets:
        - name: "{{ .Values.pullSecret }}"
      {{- end }}

      containers:
        - name: jupyter
          imagePullPolicy: IfNotPresent
          image: "{{ .Values.registry }}/{{ .Values.repo }}:{{ .Values.tag }}"
          resources:
            requests:
              memory: "{{ .Values.memory }}"
              cpu: "{{ .Values.cpu }}"
            # gotmpl's or is a function taking 2 args - this is "X or Y or Z" translated to python
            {{- if or (or .Values.gpu .Values.memoryLimit) .Values.cpuLimit }}
            limits:
            {{- if .Values.gpu }}
              nvidia.com/gpu: "1"
            {{- end }}
            {{- if .Values.cpuLimit }}
              cpu: "{{ .Values.cpuLimit }}"
            {{- end }}
            {{- if .Values.memoryLimit }}
              memory: "{{ .Values.memoryLimit }}"
            {{- end }}
            {{- end }}
          startupProbe:
            tcpSocket:
              port: 8888
            initialDelaySeconds: 5
            failureThreshold: 300  # start up totals 2 * 60 = up to 2 mins # temp, don't let me merge it
            periodSeconds: 5
          livenessProbe:
            tcpSocket:
              port: 8888
            failureThreshold: 24
            periodSeconds: 5
          ports:
            - containerPort: 8888
              name: viewer
          workingDir: "/home/{{ .Values.user }}"
          env:
            - name: NOTEBOOK_ARGS
              value: "--ServerApp.base_url=/polaris/{{ .Values.name }}/ --IdentityProvider.token= --ServerApp.root_dir=/home/{{ .Values.user }}"
            # necessary to let it run when home dirs aren't yet mounted
            - name: CHOWN_HOME
              value: "yes"
            - name: NB_USER
              value: "{{ .Values.user }}"
            - name: "NB_UID"
              value: "{{ .Values.puid }}"
            - name: "NB_GID"
              value: "{{ .Values.guid }}"
            {{- range .Values.env }}
            - name: {{ .name | quote }}
              value: {{ .value | quote }}
            {{- end }}
          volumeMounts:
            {{- if .Values.volumeMounts }}
            {{- toYaml .Values.volumeMounts | nindent 12 }}
            {{- end }}
          securityContext:
            # This is checked by the startup script and used to chown the home dir.
            # It then performs a switch to the unprivileged account.
            # The web app itself runs as the uid passed in via NB_USER, non-root 
            runAsUser: 0
      volumes:
        {{- if .Values.volumeMounts }}
        {{- toYaml .Values.volumes | nindent 8 }}
        {{- end }}
